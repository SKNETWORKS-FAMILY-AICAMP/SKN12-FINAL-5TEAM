{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d74adf8",
   "metadata": {},
   "source": [
    "## 라이브러리 임포트 및 모델 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff0ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\final_git\\SKN12-FINAL-5TEAM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 데이터 경로 (os.path.join 사용, 절대 경로)\n",
    "BASE_DIR = os.path.dirname(os.getcwd())  # ml_code 폴더의 상위 폴더로 이동\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'ML')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d24b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\final_git\\SKN12-FINAL-5TEAM\n",
      "c:\\final_git\\SKN12-FINAL-5TEAM\\data\\ML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BM-K/KoSimCSE-roberta. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BiEncoder loaded on: cpu, max_seq_length: 512\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 데이터 경로 (os.path.join 사용, 절대 경로)\n",
    "BASE_DIR = os.path.dirname(os.getcwd())  # ml_code 폴더의 상위 폴더로 이동\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'ML')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(BASE_DIR)\n",
    "print(DATA_DIR)\n",
    "\n",
    "# 디바이스 확인 (cuda가 있으면 GPU 사용)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# BiEncoder 모델 로딩\n",
    "bi_model = SentenceTransformer(\"BM-K/KoSimCSE-roberta\")\n",
    "bi_model.to(device)\n",
    "\n",
    "# (선택) 최대 문장 길이 조정하고 싶으면 아래처럼 설정 가능\n",
    "bi_model.max_seq_length = 512  # 기본은 384인데 필요 시 늘리기 가능\n",
    "\n",
    "print(f\"✅ BiEncoder loaded on: {device}, max_seq_length: {bi_model.max_seq_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a6453",
   "metadata": {},
   "source": [
    "## json 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 분할 비율 설정\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"데이터를 train, val, test로 분할합니다.\"\"\"\n",
    "    random.shuffle(data)\n",
    "    total_size = len(data)\n",
    "    \n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    \n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def load_qa_data(data, split_name=\"\"):\n",
    "\n",
    "    qas = []\n",
    "    scores = []\n",
    "    for d in tqdm(data, desc=f\"📦 Loading {split_name}\", ncols=100):\n",
    "        qas.append((d['question'], d['answer']))\n",
    "        scores.append(d['score'])\n",
    "    print(f\"✅ Loaded {len(qas)} Q&A pairs from {split_name} data.\")\n",
    "\n",
    "    SAVE_FILE_PATH = os.path.join(DATA_DIR, f\"{split_name}_data.json\")\n",
    "    with open(SAVE_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return qas, scores\n",
    "\n",
    "def embed_qas(qas, model, split_name=\"\"):\n",
    "    print(f\"🚀 Embedding {split_name}...\")\n",
    "\n",
    "    questions = [q for q, _ in qas]\n",
    "    answers = [a for _, a in qas]\n",
    "\n",
    "    # 각 문장 리스트를 임베딩\n",
    "    q_vecs = model.encode(\n",
    "        questions,\n",
    "        convert_to_numpy=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    a_vecs = model.encode(\n",
    "        answers,\n",
    "        convert_to_numpy=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # 벡터 이어붙이기 (concat)\n",
    "    features = [list(q) + list(a) for q, a in tqdm(zip(q_vecs, a_vecs), total=len(qas), desc=f\"🔗 Merging {split_name}\", ncols=100)]\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e923f",
   "metadata": {},
   "source": [
    "## question과 answer 각각 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d833af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data split complete - Train: 5257, Val: 657, Test: 658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📦 Loading train: 100%|█████████████████████████████████████████████████| 5257/5257 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5257 Q&A pairs from train data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📦 Loading val: 100%|█████████████████████████████████████████████████████| 657/657 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 657 Q&A pairs from val data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📦 Loading test: 100%|█████████████████████████████████████████| 658/658 [00:00<00:00, 89324.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 658 Q&A pairs from test data.\n",
      "🚀 Embedding train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches: 100%|██████████| 165/165 [02:08<00:00,  1.28it/s]\n",
      "\n",
      "Batches: 100%|██████████| 165/165 [23:45<00:00,  8.64s/it]\n",
      "🔗 Merging train:   0%|                                                    | 0/5257 [00:00<?, ?it/s]\n",
      "🔗 Merging train: 100%|██████████████████████████████████████| 5257/5257 [00:00<00:00, 10059.59it/s]\n",
      "🔗 Merging train: 100%|██████████████████████████████████████| 5257/5257 [00:00<00:00, 10059.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Embedding val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 21/21 [00:17<00:00,  1.19it/s]\n",
      "Batches:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 21/21 [02:25<00:00,  6.92s/it]\n",
      "Batches: 100%|██████████| 21/21 [02:25<00:00,  6.92s/it]                    | 0/657 [00:00<?, ?it/s]\n",
      "🔗 Merging val: 100%|██████████████████████████████████████████| 657/657 [00:00<00:00, 10558.04it/s]\n",
      "🔗 Merging val: 100%|██████████████████████████████████████████| 657/657 [00:00<00:00, 10558.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Embedding test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 21/21 [00:18<00:00,  1.14it/s]\n",
      "Batches:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 21/21 [02:25<00:00,  6.93s/it]\n",
      "Batches: 100%|██████████| 21/21 [02:25<00:00,  6.93s/it]\n",
      "🔗 Merging test: 100%|██████████████████████████████████████████| 658/658 [00:00<00:00, 5393.76it/s]\n",
      "🔗 Merging test: 100%|██████████████████████████████████████████| 658/658 [00:00<00:00, 5393.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessed_data.json 로드\n",
    "with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, val_data, test_data = split_data(processed_data, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
    "\n",
    "print(f\"✅ Data split complete - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# 호출 시 split 이름 명시\n",
    "train_qa, y_train = load_qa_data(train_data, split_name=\"train\")\n",
    "val_qa, y_val = load_qa_data(val_data, split_name=\"val\")\n",
    "test_qa, y_test = load_qa_data(test_data, split_name=\"test\")\n",
    "\n",
    "X_train = embed_qas(train_qa, bi_model, split_name=\"train\")\n",
    "X_val = embed_qas(val_qa, bi_model, split_name=\"val\")\n",
    "X_test = embed_qas(test_qa, bi_model, split_name=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefa728",
   "metadata": {},
   "source": [
    "## array로 변환 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640cdd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# numpy array로 변환 \n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 데이터 저장\n",
    "np.savez(os.path.join(DATA_DIR, \"train_set.npz\"), X=X_train, y=y_train)\n",
    "np.savez(os.path.join(DATA_DIR, \"val_set.npz\"), X=X_val, y=y_val)\n",
    "np.savez(os.path.join(DATA_DIR, \"test_set.npz\"), X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbed6ec",
   "metadata": {},
   "source": [
    "## 저장한 array 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb2c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(os.path.join(DATA_DIR, \"train_set.npz\"))\n",
    "X_train = train[\"X\"]\n",
    "y_train = train[\"y\"]\n",
    "\n",
    "val = np.load(os.path.join(DATA_DIR, \"val_set.npz\"))\n",
    "X_val = val[\"X\"]\n",
    "y_val = val[\"y\"]\n",
    "\n",
    "test = np.load(os.path.join(DATA_DIR, \"test_set.npz\"))\n",
    "X_test = test[\"X\"]\n",
    "y_test = test[\"y\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handspell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
